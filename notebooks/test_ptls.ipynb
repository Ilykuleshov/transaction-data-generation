{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles import ColesDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_parquet('data/new_data/preprocessed/preproc_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df.drop(columns=['sample_label', 'target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>transaction_amt</th>\n",
       "      <th>transaction_dttm</th>\n",
       "      <th>is_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.889078</td>\n",
       "      <td>2020-08-03 08:05:23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.922270</td>\n",
       "      <td>2020-08-05 01:27:40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.933393</td>\n",
       "      <td>2020-08-05 03:28:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.734882</td>\n",
       "      <td>2020-08-06 00:36:29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.893904</td>\n",
       "      <td>2020-08-09 00:30:13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160987</th>\n",
       "      <td>22533</td>\n",
       "      <td>1</td>\n",
       "      <td>3.706910</td>\n",
       "      <td>2021-07-31 05:33:03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160988</th>\n",
       "      <td>22533</td>\n",
       "      <td>1</td>\n",
       "      <td>5.625801</td>\n",
       "      <td>2021-07-31 08:57:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160989</th>\n",
       "      <td>22533</td>\n",
       "      <td>1</td>\n",
       "      <td>4.927959</td>\n",
       "      <td>2021-07-31 08:59:33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160990</th>\n",
       "      <td>22533</td>\n",
       "      <td>11</td>\n",
       "      <td>4.454891</td>\n",
       "      <td>2021-08-01 23:04:41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160991</th>\n",
       "      <td>22533</td>\n",
       "      <td>83</td>\n",
       "      <td>8.489627</td>\n",
       "      <td>2021-08-02 09:20:43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19139004 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  mcc_code  transaction_amt    transaction_dttm  is_income\n",
       "0               1         1         5.889078 2020-08-03 08:05:23          0\n",
       "1               1         2         4.922270 2020-08-05 01:27:40          0\n",
       "2               1         2         4.933393 2020-08-05 03:28:11          0\n",
       "3               1         3         5.734882 2020-08-06 00:36:29          0\n",
       "4               1         1         4.893904 2020-08-09 00:30:13          0\n",
       "...           ...       ...              ...                 ...        ...\n",
       "19160987    22533         1         3.706910 2021-07-31 05:33:03          0\n",
       "19160988    22533         1         5.625801 2021-07-31 08:57:02          0\n",
       "19160989    22533         1         4.927959 2021-07-31 08:59:33          0\n",
       "19160990    22533        11         4.454891 2021-08-01 23:04:41          0\n",
       "19160991    22533        83         8.489627 2021-08-02 09:20:43          0\n",
       "\n",
       "[19139004 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mcc_code\n",
       "1      5654775\n",
       "2      1737826\n",
       "4      1577361\n",
       "9       837687\n",
       "11      687291\n",
       "        ...   \n",
       "333          1\n",
       "320          1\n",
       "319          1\n",
       "300          1\n",
       "377          1\n",
       "Name: count, Length: 377, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_df['mcc_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = PandasDataPreprocessor(\n",
    "    'user_id',\n",
    "    'transaction_dttm',\n",
    "    cols_category=['mcc_code', 'is_income'],\n",
    "    cols_numerical=['transaction_amt'],\n",
    "    return_records=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocessor.fit_transform(orig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sorted(dataset, key=lambda x: x['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 1,\n",
       " 'event_time': tensor([1596441923, 1596590860, 1596598091, 1596674189, 1596933013, 1596935908,\n",
       "         1597016604, 1597030018, 1598297326, 1598492791, 1598579687, 1598588546,\n",
       "         1598682722, 1598763249, 1598827950, 1598906774, 1598919551, 1599007728,\n",
       "         1599011081, 1599090187, 1599187559, 1599337080, 1599343800, 1599690236,\n",
       "         1599691008, 1599807011, 1599807250, 1599807291, 1599812334, 1599877249,\n",
       "         1599880784, 1599961992, 1599974942, 1600029302, 1600216611, 1600295886,\n",
       "         1600395559, 1600397605, 1600591385, 1600634580, 1600648099, 1600669921,\n",
       "         1600726127, 1600738072, 1600807681, 1600823951, 1600981153, 1601002731,\n",
       "         1601006665, 1601006964, 1601010742, 1601109529, 1601263755, 1601340163,\n",
       "         1601344889, 1601353268, 1601423401, 1601433062, 1601526411, 1601607303,\n",
       "         1602015600, 1602037995, 1602102240, 1602199480, 1602201519, 1602209422,\n",
       "         1602273000, 1602295888, 1602297346, 1602300458, 1602300714, 1602304057,\n",
       "         1602304835, 1602385471, 1602386582, 1602392114, 1602465108, 1602479252,\n",
       "         1602541552, 1602565234, 1602568610, 1602641039, 1602647657, 1602711190,\n",
       "         1602725899, 1602971340, 1603016092, 1603142039, 1603163391, 1603232847,\n",
       "         1603246764, 1603322244, 1603325246, 1603400025, 1603406317, 1603418879,\n",
       "         1603419475, 1603419507, 1603425646, 1603583134, 1603586653, 1603657353,\n",
       "         1603661204, 1603755744, 1603764064, 1603857381, 1603860275, 1604176560,\n",
       "         1604176620, 1604181780, 1604198108, 1604538143, 1604539139, 1604545079,\n",
       "         1604546479, 1604615003, 1604628850, 1604634368, 1604954880, 1604957160,\n",
       "         1604961667, 1604967859, 1604967907, 1604972621, 1604975003, 1604976916,\n",
       "         1604990304, 1605131453, 1605177940, 1605740241, 1606009825, 1606176235,\n",
       "         1606356974, 1606882057, 1606882150, 1607038612, 1607041521, 1607043349,\n",
       "         1607045266, 1607309109, 1607324933, 1607572363, 1607576812, 1607577731,\n",
       "         1607579086, 1607581844, 1607643605, 1607656792, 1607751134, 1607831692,\n",
       "         1608091893, 1608094380, 1608175161, 1608176719, 1608185806, 1608186699,\n",
       "         1608324600, 1608328800, 1608342413, 1608347413, 1608348061, 1608348515,\n",
       "         1608351577, 1608408540, 1608417930, 1608423075, 1608423798, 1608430056,\n",
       "         1608438194, 1608606758, 1608611884, 1608614580, 1608670359, 1608673140,\n",
       "         1608707429, 1608708135, 1608709008, 1608757860, 1608760800, 1608859153,\n",
       "         1608933124, 1608935359, 1608938408, 1608940472, 1608945016, 1609020540,\n",
       "         1609028079, 1609029789, 1609040451, 1609196366, 1609198415, 1609202764,\n",
       "         1609206885, 1609210877, 1609308470, 1609381856, 1609489149, 1609801920,\n",
       "         1609803042, 1609819046, 1610050560, 1610056200, 1610221920, 1610567460,\n",
       "         1610571011, 1610599322, 1610614290, 1610679058, 1610753284, 1610758786,\n",
       "         1610777123, 1611007130, 1611029119, 1611527612, 1611637143, 1611701339,\n",
       "         1611714292, 1611785958, 1611811274, 1611909684, 1612129797, 1612140541,\n",
       "         1612142561, 1612144347, 1612146092, 1612146378, 1612150499, 1612744932,\n",
       "         1612915368, 1612927127, 1612927692, 1612937236, 1612938432, 1612940469,\n",
       "         1612999095, 1612999772, 1613003800, 1613004588, 1613007483, 1613008625,\n",
       "         1613040057, 1613083744, 1613088845, 1613096965, 1613181324, 1613185289,\n",
       "         1613187317, 1613346301, 1613353780, 1613368410, 1613441543, 1613455628,\n",
       "         1613513553, 1613518577, 1613797108, 1613802505, 1613903755, 1614045556,\n",
       "         1614050748, 1614113236, 1614113555, 1614127215, 1614216732, 1614218616,\n",
       "         1614224857, 1614237060, 1614303271, 1614311962, 1614323495, 1614543605,\n",
       "         1614561543, 1614565585, 1614570581, 1614631810, 1614635434, 1614651352,\n",
       "         1614723738, 1614740553, 1614742672, 1614828774, 1614831156, 1614889206,\n",
       "         1614924294, 1614927970, 1614939189, 1614993523, 1615074271, 1615079327,\n",
       "         1615081575, 1615082064, 1615084920, 1615087558, 1615183213, 1615189562,\n",
       "         1615238943, 1615248102, 1615248429, 1615253518, 1615255352, 1615258296,\n",
       "         1615325942, 1615329090, 1615331875, 1615337294, 1615352977, 1615408985,\n",
       "         1615420628, 1615425160, 1615426719, 1615497149, 1615498320, 1615509207,\n",
       "         1615510243, 1615513078, 1615520913, 1615523018, 1615526957, 1615604652,\n",
       "         1615606393, 1615688056, 1615692371, 1615696710, 1615709758, 1615771236,\n",
       "         1615775759, 1615842092, 1615845446, 1615867748, 1615926180, 1615948566,\n",
       "         1615953932, 1615956516, 1615962690, 1616100838, 1616118507, 1616125745,\n",
       "         1616147603, 1616359382, 1616372107, 1616372629, 1616377003, 1616387796,\n",
       "         1616450080, 1616462932, 1616550658, 1616551377, 1616563600, 1616625546,\n",
       "         1616630047, 1616633952, 1616635756, 1616636955, 1616805442, 1616811683,\n",
       "         1616819917, 1616819978, 1616919663, 1616994620, 1617058690, 1617074714,\n",
       "         1617139444, 1617152721, 1617153136, 1617155702, 1617164630, 1617165973,\n",
       "         1617167201, 1617229310, 1617234313, 1617253877, 1617313649, 1617319195,\n",
       "         1617321101, 1617327116, 1617337943, 1617421143, 1617424110, 1617487869,\n",
       "         1617488403, 1617491139, 1617494206, 1617498684, 1617502509, 1617505438,\n",
       "         1617513178, 1617651486, 1617658445, 1617660624, 1617666392, 1617666964,\n",
       "         1617672136, 1617686535, 1617686613, 1617743226, 1617774034, 1617775386,\n",
       "         1617823510, 1617832906, 1617845306, 1617854658, 1617914528, 1617924030,\n",
       "         1617924537, 1617925628, 1617940845, 1618010486, 1618208863, 1618264441,\n",
       "         1618289165, 1618290133, 1618293239, 1618302884, 1618354926, 1618364004,\n",
       "         1618365936, 1618433154, 1618440128, 1618440483, 1618450364, 1618451189,\n",
       "         1618463070, 1618516936, 1618518247, 1618540954, 1618549285, 1618561068,\n",
       "         1618620112, 1618620186, 1618623376, 1618636824, 1618711999, 1618712927,\n",
       "         1618793384, 1618809599, 1618816100, 1618868461, 1618869190, 1618953789,\n",
       "         1618960220, 1618967428, 1618984830, 1619035260, 1619037969, 1619062923,\n",
       "         1619121191, 1619129869, 1619155760, 1619207384, 1619215980, 1619231729,\n",
       "         1619235887, 1619320758, 1619328146, 1619328268, 1619380331, 1619397482,\n",
       "         1619398073, 1619410676, 1619411432, 1619411548, 1619412222, 1619413113,\n",
       "         1619416457, 1619493610, 1619494929, 1619551389, 1619567102, 1619567215,\n",
       "         1619596253, 1619645351, 1619651778, 1619723828, 1619736787, 1619764662,\n",
       "         1619773551, 1619815964, 1619817725, 1619817854, 1619818058, 1619820207,\n",
       "         1619898900, 1619899860, 1619983383, 1619991183, 1619992512, 1620012088,\n",
       "         1620016355, 1620020825, 1620105006, 1620181758, 1620182753, 1620183449,\n",
       "         1620264511, 1620266673, 1620274298, 1620354583, 1620358581, 1620359035,\n",
       "         1620430269, 1620502552, 1620529811, 1620534253, 1620534730, 1620540121,\n",
       "         1620607407, 1620611546, 1620673805, 1620693977, 1620707052, 1620707569,\n",
       "         1620766690, 1620774529, 1620777690, 1620778006, 1620778023, 1620792095,\n",
       "         1620851167, 1620882190, 1620935048, 1620946619, 1620951532, 1620954819,\n",
       "         1620964567, 1621117926, 1621123951, 1621124801, 1621125520, 1621126254,\n",
       "         1621126500, 1621131729, 1621137094, 1621193345, 1621220910, 1621222212,\n",
       "         1621280407, 1621281780, 1621302159, 1621391996, 1621398500, 1621403311,\n",
       "         1621453747, 1621468520, 1621481459, 1621490183, 1621502509, 1621547534,\n",
       "         1621551426, 1621562225, 1621573086, 1621574223, 1621587242, 1621655349,\n",
       "         1621655882, 1621666482, 1621667019, 1621667703, 1621721675, 1621722888,\n",
       "         1621723291, 1621748334, 1621798082, 1621809325, 1621832683, 1621834603,\n",
       "         1621886168, 1621975982, 1622004244, 1622063763, 1622067163, 1622076788,\n",
       "         1622077322, 1622084794, 1622092869, 1622151960, 1622156581, 1622156941,\n",
       "         1622161609, 1622161645, 1622162381, 1622164116, 1622164446, 1622165476,\n",
       "         1622166453, 1622167855, 1622169724, 1622231460, 1622236680, 1622237355,\n",
       "         1622264829, 1622268729, 1622346538, 1622408283, 1622408626, 1622420221,\n",
       "         1622426744, 1622436299, 1622439504, 1622488564, 1622508939, 1622576462,\n",
       "         1622608525, 1622609422, 1622668023, 1622749986, 1622764747, 1622766246,\n",
       "         1622779806, 1622780051, 1622862997, 1622863055, 1622864027, 1622950983,\n",
       "         1622952177, 1622961962, 1622962969, 1622963871, 1623019200, 1623039194,\n",
       "         1623043028, 1623097982, 1623117429, 1623122885, 1623132547, 1623137046,\n",
       "         1623184920, 1623196490, 1623208627, 1623208770, 1623272407, 1623281100,\n",
       "         1623286979, 1623291069, 1623315609, 1623354604, 1623362518, 1623372220,\n",
       "         1623380980, 1623466606, 1623544568, 1623544883, 1623545026, 1623550163,\n",
       "         1623553183, 1623554491, 1623647154, 1623647268, 1623786540, 1623791226,\n",
       "         1623799557, 1623876120, 1623879063, 1623893737, 1623893782, 1623901266,\n",
       "         1623969470, 1623974695, 1623975743, 1623990102, 1623992101, 1623996599,\n",
       "         1624078723, 1624144503, 1624146260, 1624147717, 1624151724, 1624151920,\n",
       "         1624154350, 1624245163, 1624246576, 1624246632, 1624311225, 1624334110,\n",
       "         1624389304, 1624407711, 1624414357, 1624417048, 1624484042, 1624486229,\n",
       "         1624507482, 1624580824, 1624675039, 1624748525, 1624765700, 1624769552,\n",
       "         1624772800, 1624774327, 1624820705, 1624826820, 1624828161, 1624851965,\n",
       "         1624858679, 1624861854, 1624915445, 1624920965, 1624923718, 1624928286,\n",
       "         1624928534, 1624935002, 1624948330, 1624950027, 1624956187, 1624999622,\n",
       "         1625010049, 1625013892, 1625080562, 1625093003, 1625098501, 1625101141,\n",
       "         1625109523, 1625109805, 1625118494, 1625175399, 1625188984, 1625201350,\n",
       "         1625284486, 1625367450, 1625426343, 1625450773, 1625453229, 1625520851,\n",
       "         1625526489, 1625542807, 1625605502, 1625629637, 1625707266, 1625792853,\n",
       "         1625793836, 1625796708, 1625797532, 1625808470, 1625860080, 1625867173,\n",
       "         1625871855, 1625872167, 1625875221, 1625875928, 1625876107, 1625880006,\n",
       "         1625887975, 1625899815, 1625900123, 1625900656, 1625902885, 1625958789,\n",
       "         1625959299, 1625965430, 1625982257, 1625983578, 1626046652, 1626125161,\n",
       "         1626213122, 1626236834, 1626296533, 1626297302, 1626300254, 1626301974,\n",
       "         1626314319, 1626316512, 1626396616, 1626412104, 1626412698, 1626475801,\n",
       "         1626565886, 1626572050, 1626589321, 1626597877, 1626663658, 1626749827,\n",
       "         1626833476, 1626854574, 1626900121, 1626909195, 1626998106, 1627003369,\n",
       "         1627081729, 1627086690, 1627185237, 1627186225, 1627188562, 1627243441,\n",
       "         1627333925, 1627357355, 1627362038, 1627413843, 1627426384, 1627452220,\n",
       "         1627505104, 1627519273, 1627532104, 1627537975, 1627586826, 1627597868,\n",
       "         1627607149, 1627607650, 1627609454, 1627690653, 1627693592, 1627693910,\n",
       "         1627695774]),\n",
       " 'mcc_code': tensor([  1,   2,   2,  12,   1,   3,  91,   1,   3,   1,  13,   1,   2,   1,\n",
       "           3,   3,   1,   2, 145,   1,   1,   6,   4,   3,  10,   5,  79,   1,\n",
       "          12,   1,   2,  13,   1,   3,  17,  71,   1,   1,   1,  79,   3,  12,\n",
       "          71,   2,   3,   1,   3,   2,  32,  36,   2,   1,  32,   3,  12,   1,\n",
       "           3,  17, 145,   1,  16, 145,  16,   3,  16,  12,  21,   2,   1,   1,\n",
       "           2,   2,   1,  32,   1,   6,  13,   1,   3,   5,  12,   3,   1,   3,\n",
       "           2, 112,  12,   3,   1,   3,   1,   3,   3,   3,   3,   2,   1,   3,\n",
       "           1,   7, 145,   5,   3,   3,   1,  32,   1,   2,   2,   1,   7,   1,\n",
       "           2,  32,  13,   3,  12,   5,   8,   8,  12,  13,   1,   7,  32,   5,\n",
       "          12,   3,  12, 145,   7,   3,   1,   7,   7,  79,   7,   1,   3,   7,\n",
       "          12,   3,   5,   1,   1, 108, 145,   2,  13,  13,   1,   2,  13,  18,\n",
       "           1,   5,  32, 145,  16,   5,  32, 105, 105,   5,   2,   1,   3,  13,\n",
       "           1,  48,   2,   1,   2,   3,  12,  16,  16,   2,   1,  12,  79,   1,\n",
       "         105,   2,   1,  46,  18,  13,   2,   1,  14,   2,  17,  17,   5,   2,\n",
       "          12,   1,   2,   1, 145,   1,  91,  48,  19,  29,  12,   1,   3,   1,\n",
       "           4,  19,   2,   3, 145,   3,   2,   3,  18,   1,   3,  67,   1,  91,\n",
       "           7,   1,   2,   7,   5,  56,   3, 145,  14,  14,   1,   7,   7,   3,\n",
       "          73,  32,  12,   8,   1,   8,   1,  13,   2,   7,  56,   5,   1,   1,\n",
       "           3, 145,   8,   1,  12,  18,   1,   3,   3,  20,   2, 145,   8,   1,\n",
       "          27,   8,   1,   8,  56,   8,   8,   8,   3,   2,   8,  16,   8,  13,\n",
       "           5,   8,   1,   7,  12,  13,  22,   1,   8,   8,  18,  56,   8,   8,\n",
       "           8,  56,   8,   2,  27,   8,   8,  32,   2,   8,   1,   8,   1,   1,\n",
       "           7,   8,  13,   3,  43,   8,   8,   1,   8,   1,   8,   8,   8,   8,\n",
       "           5,  12,   8,   8,   8,   8,  72,   8,   8,   8,   8,   8,  20,   8,\n",
       "          12,   8,   8,  36,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,\n",
       "           8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8,   8, 145,   2,\n",
       "           8,  32,   8,   8,   3,   1,   3,  20,   3,  36,   8,   1,  13,  36,\n",
       "           8,  13,  13,   2,   8,   1,   1,   8,   3,   3,  36,   3,   4,   3,\n",
       "           8,   8,   4,   1,   8,   3,  46,   2,   8,   3,   3, 145,   8,   3,\n",
       "           8,   3,  18,   2,   8,   8,   3,   8,   8,   1,   8,   5,   7,   7,\n",
       "           8,   8,   8,   8,   2,   8,  32,   1,  27,   1,   8,   3,   3,   8,\n",
       "           8,   3,   8,   8,   7,   1,   8,  20,   8,   8,   8,   3,   8,   8,\n",
       "         145,   1,  16,  79,   1,   1,   8,   3,  36,   8,   8,  64,  64,   8,\n",
       "           2,   1,   5,   8,   3,   3,   8,   8,   3,   8,   3,   5,   1,   1,\n",
       "           1,  22,   2,   2,   6,   6,   8,   8,   8,  22,   5,   8,   4,   8,\n",
       "          27,   1,  22,   2,   1,  51,   8,   2,  42,   8,   8,   8,   8,   2,\n",
       "          42,  29,   8,   3,  22,  22,   8,   3,   3,   3,   1,   5,   8,   8,\n",
       "           8,   3,  12,   5,   1,   8,  27,   1,  32,   2,  56,   8,  12,   8,\n",
       "           8,   1,   8,  20,  20,   1,   8,   1,   8,   1,   1,   8,  12,   3,\n",
       "           3,   1,   8,   1,   8,   2,   8,  32,  27,   8,  13,   5,   1,   1,\n",
       "           8,   3,   1,   3,   8,   8,   8,   8,  27,   3, 145,  51,   8,  20,\n",
       "         145,   2,   3,   1,   1,  56,   8,  32,   1, 145, 145,   8,   8,  22,\n",
       "           8,   8,   3,   8,  27,  36,   5,   2,   1,   8,   1,   8,   7,   8,\n",
       "           8,   8,   1,   1,   1,   8,   1,   3,   1,   8,  12,   8,   2,   1,\n",
       "           3,   1,   8,   8,   8,   2,   1,   1,  20,  36,   8,   2,   8,  51,\n",
       "           3,   8,   1,   8,   3,  18,   1,   1,   1,  27,  32,   5,   1,   5,\n",
       "           2,   2,  20,   8,   3,  20,   8,  32,  56,   8,   1,   5,   3,   1,\n",
       "           8,   2,   1,   8,  22,  22,   2,  56,   8,   3,  22,  27,  27,   8,\n",
       "           8,   3,   2,   1,   8,  27,   2,   8,   5,   5, 145,   1,   2,   8,\n",
       "           8,  36,  27,   8,   2, 145,   8,  20,  32,   3,  32,   8,   1,  13,\n",
       "          12,   8,   3,   3,   8,  27,   8,   8,  27,  32,   1,   8,   8,   8,\n",
       "           1,   2,   8,   3, 145,   8,   5,   2,   8,   2, 145,   8,  52,   1,\n",
       "           8,   1,  13,   8,   8,  43,   8,   8,   8,  14,   1,   8,   8,   8,\n",
       "           8,   1,  22,  22,   8,   8,   1,   8,   8,   2,   1,   8,  91,   1,\n",
       "           1,   5,  12,  55,  27,   8,   1,   8,   7,   1,   2,   1,   1,  12,\n",
       "           8,   8,   1,   1,   1,   2,   1,   1,  13,   8,   8,   3,   2,   8,\n",
       "           8,   1,   8,   1,   1,  22,   8,   5,   2,   1,  27,  76,  15,   5,\n",
       "          15]),\n",
       " 'is_income': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "         1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]),\n",
       " 'transaction_amt': tensor([ 5.8891,  4.9223,  4.9334,  5.7349,  4.8939,  4.1497,  7.4026,  7.3118,\n",
       "          5.1768,  4.5548,  6.3328,  5.2975,  6.1319,  4.7295,  5.1310,  3.5500,\n",
       "          5.2789,  4.5874,  5.0824,  6.0758,  5.3426,  7.0328,  5.8719,  4.1416,\n",
       "          4.2029,  6.3351,  5.7097,  5.7884,  7.1899,  5.3134,  6.1982,  4.8893,\n",
       "          4.7977,  4.6451,  5.4947,  6.5911,  5.2592,  5.1977,  6.0263,  9.3548,\n",
       "          4.6839,  6.8162,  6.0626,  4.3095,  4.0837,  5.3076,  3.5665,  4.1829,\n",
       "          4.9901,  4.8471,  4.0942,  5.8825,  4.3663,  4.0080,  6.9759,  5.5557,\n",
       "          4.1902,  5.4659,  7.1682,  5.6762,  7.6756,  5.9340,  7.0192,  5.9446,\n",
       "          5.0910,  8.1033, 10.3073,  6.6292,  5.3820,  4.9366,  4.8465,  5.2916,\n",
       "          7.8412,  6.7976,  5.8247,  6.9875,  5.8068,  5.6775,  4.9406, 10.4392,\n",
       "          3.7991,  4.6585,  5.2424,  4.8335,  5.7739,  7.9691,  7.0404,  4.7853,\n",
       "          3.8581,  4.8115,  4.2151,  5.4035,  5.3390,  2.6551,  4.5075,  5.9858,\n",
       "          4.4969,  5.6722,  5.6684,  6.0256,  6.5291,  8.5246,  5.3268,  3.9836,\n",
       "          5.1727,  7.4249,  6.5460,  5.8211,  4.7625,  8.0225,  4.6423,  4.5176,\n",
       "          4.6186,  4.6509,  4.9435,  5.0951,  5.9327,  7.2203,  3.1776,  3.3865,\n",
       "          8.5339,  4.5726,  3.3426,  6.0025,  4.3380,  5.3681,  5.0319,  4.9163,\n",
       "          8.5973,  6.6995,  6.0582,  5.2743,  6.4073,  5.5124,  7.4485,  5.0314,\n",
       "          4.9289,  5.2695,  5.2197,  6.3627,  7.5440,  5.0637,  5.8995,  7.0701,\n",
       "          5.5458,  7.1512,  6.8729,  6.2778,  5.6688,  8.3238,  5.5103,  5.3358,\n",
       "          5.9353,  5.5772,  6.4874,  6.4646,  6.4558,  9.0705,  6.7406,  5.3647,\n",
       "          5.6712,  5.1626,  5.9287,  6.9740,  5.5616,  5.0668,  3.9489,  4.8056,\n",
       "          6.6760,  4.1531,  4.8446,  6.5283,  5.4437,  4.8888,  8.6278,  4.7942,\n",
       "          5.2921,  6.4883,  6.3155, -0.1993,  6.4531,  6.0400,  8.2316,  6.1953,\n",
       "          6.8428,  7.6591,  6.1420,  5.8278,  5.3683,  5.6885,  6.4062,  5.3431,\n",
       "          4.1531,  5.9763,  6.1492,  6.1985,  8.7344,  5.9177,  4.4250,  6.9339,\n",
       "          6.0288,  7.3968,  5.0700,  7.4565,  6.6191, 10.2582,  7.9266,  6.8503,\n",
       "          5.5388,  6.8888,  5.7106,  6.3734,  5.1226,  6.5659,  6.5850,  6.6195,\n",
       "          5.5090,  4.8712,  5.0726,  6.6731,  3.3670,  6.0726,  5.5318,  5.8684,\n",
       "          4.3147,  6.2567,  6.0120,  4.2536,  7.5405,  5.6946,  4.7631,  6.1261,\n",
       "          5.4128,  5.2082,  4.8193,  5.4843,  4.3734,  4.1479,  5.3921,  4.6209,\n",
       "          7.8905,  3.4170,  6.2921,  3.4046,  6.7226,  4.6061,  5.3292,  6.1059,\n",
       "          4.3417,  9.1542,  5.7281,  4.9486,  4.0335,  5.8417,  3.2801,  4.8763,\n",
       "          7.9635,  3.3180,  5.6453,  4.3128,  5.4595,  7.1673,  5.1622,  4.9381,\n",
       "          3.3892,  4.4051,  4.7614,  3.3968,  5.6227,  3.1774,  5.3137,  3.1097,\n",
       "          3.1127,  3.0572,  3.8336,  5.1555,  3.2000,  4.0117,  3.2140,  4.8441,\n",
       "          6.4946,  3.0581,  5.7431,  6.2029,  7.9027,  7.0328,  6.0285,  5.4228,\n",
       "          3.0134,  3.2240,  5.1952,  4.7969,  3.0574,  3.0192,  3.1965,  5.2562,\n",
       "          3.2086,  5.7971,  4.8186,  3.1918,  3.2353,  5.1244,  5.2253,  3.2239,\n",
       "          5.9284,  3.0757,  5.3999,  4.2738,  4.4873,  3.0911,  5.9280,  3.9976,\n",
       "          4.3084,  3.2017,  3.2122,  5.3438,  3.2076,  6.7827,  3.0100,  3.0462,\n",
       "          3.0808,  3.1063,  9.4104,  6.6934,  3.0382,  3.0315,  3.1140,  3.0754,\n",
       "         10.2707,  3.2084,  3.2001,  3.0811,  3.0591,  3.0601,  6.4954,  3.2232,\n",
       "          8.6244,  3.0780,  3.2437,  5.1646,  3.1748,  3.0961,  3.1088,  3.0521,\n",
       "          3.1721,  3.0714,  3.0411,  3.1860,  3.1109,  3.2426,  3.0205,  3.0811,\n",
       "          3.0550,  3.0288,  3.0618,  3.2439,  3.2192,  3.2228,  3.0863,  3.0740,\n",
       "          3.2012,  3.1657,  5.4547,  5.9394,  3.0532,  5.1609,  3.2016,  3.2215,\n",
       "          5.8534,  4.5314,  4.3855,  6.6898,  4.7631,  5.3720,  3.0483,  6.5047,\n",
       "          6.6590,  5.4641,  3.0287,  6.1305,  4.8863,  6.9489,  3.2315,  2.4008,\n",
       "          6.1649,  3.0248,  3.5644,  2.2949,  5.2043,  4.6483,  6.4271,  4.5285,\n",
       "          3.0321,  3.0196,  5.8170,  6.9664,  3.2124,  4.5697,  7.9357,  5.9452,\n",
       "          3.2380,  4.1466,  4.7974,  6.0586,  3.0818,  4.8363,  3.2371,  4.7600,\n",
       "          4.0197,  5.7740,  3.1909,  3.2358,  5.3783,  3.0106,  3.0578,  5.7054,\n",
       "          3.0594,  5.6062,  5.5326,  5.1380,  3.0559,  3.2196,  3.0367,  3.2067,\n",
       "          5.7798,  3.2766,  5.3231,  3.7332,  5.1814,  5.4113,  3.0066,  6.3586,\n",
       "          4.1117,  3.2125,  3.0488,  4.6732,  3.0674,  3.1025,  7.0713,  4.3092,\n",
       "          3.1860,  7.5892,  3.0285,  2.9973,  3.1693,  5.1239,  3.1930,  3.2888,\n",
       "          6.4196,  6.4104,  2.3317,  3.4324,  6.0984,  3.9033,  3.2365,  4.4419,\n",
       "          5.8774,  3.2071,  3.0824,  5.9823,  2.6936,  3.1792,  4.0973,  6.1908,\n",
       "          7.5452,  3.0128,  3.5949,  4.2203,  3.0901,  3.2029,  4.7537,  3.2238,\n",
       "          5.1173,  9.7171,  5.9056,  6.0570,  4.9941,  5.6509,  4.4491,  5.2754,\n",
       "          9.4415,  9.8484,  3.2258,  3.1913,  3.1149,  6.4613,  6.1370,  3.2366,\n",
       "          6.1836,  3.1984,  3.9623,  6.3954,  5.2139,  6.3009,  6.0402,  3.6743,\n",
       "          3.2485,  4.2769,  7.4020,  3.3338,  3.0077,  3.2019,  3.0451,  4.0004,\n",
       "          2.2985,  5.8495,  3.0813,  6.0077,  5.2940,  6.0324,  3.1787,  4.9768,\n",
       "          3.6999,  4.7940,  5.6122, 10.5037,  3.2303,  3.2000,  3.0484,  3.7192,\n",
       "          5.6406,  9.4538,  7.2330,  3.2196,  4.5731,  5.4501,  5.6228,  6.7346,\n",
       "          5.6557,  3.2225,  7.6687,  3.1068,  3.0720,  5.4880,  3.0297,  8.1512,\n",
       "          7.7316,  5.5749,  3.0343,  5.8232,  3.1849,  5.1315,  4.0916,  3.2298,\n",
       "          7.9976,  3.8975,  4.2354,  4.8759,  3.0881,  4.3550,  3.2205,  4.8111,\n",
       "          3.0619,  6.0582,  4.3808,  3.1715,  5.0382,  7.1014,  3.7868,  6.4953,\n",
       "          3.2001,  4.4100,  6.6359,  4.4810,  3.2552,  3.2347,  2.9973,  3.2485,\n",
       "          4.4272,  5.1254,  6.6744,  4.6992,  3.2200,  8.8237,  5.9501,  4.6536,\n",
       "          5.0807,  4.7658,  4.5327,  5.7426,  3.0790,  5.1561,  7.4081,  6.7432,\n",
       "          6.6598,  3.2982,  3.4546,  5.2720,  3.1978,  3.2309,  5.6811,  3.0370,\n",
       "          4.3660,  4.5422,  8.4466,  3.6921,  5.7683,  3.0406,  5.0260,  3.2549,\n",
       "          6.3341,  3.1979,  3.0386,  3.0680,  6.1172,  6.4636,  5.6310,  3.2440,\n",
       "          6.3196,  5.9335,  6.0236,  3.1797,  7.7985,  3.1919,  5.0289,  5.6545,\n",
       "          5.0383,  5.2986,  3.1901,  3.0986,  3.0294,  5.1227,  5.2796,  4.6967,\n",
       "          8.6094,  4.0135,  3.0347,  5.7016,  3.1531,  4.1541,  5.1946,  3.0899,\n",
       "          4.7726,  3.0559,  4.0955,  3.9927,  5.3215,  6.7879,  4.1772,  4.5852,\n",
       "          5.2020,  8.5667,  6.5743, 10.0743,  4.8258,  5.3682,  8.0274,  3.1826,\n",
       "          5.4901,  8.1829,  3.0362,  4.5165,  5.3499,  3.0914,  5.4317,  5.8961,\n",
       "          4.7759,  5.9489,  2.9934,  5.1725,  6.3075,  3.0152,  6.2304,  3.6314,\n",
       "          5.9844,  5.6612,  3.0658,  6.5934,  6.4622,  4.8324,  4.1444,  3.0068,\n",
       "          3.2466,  4.4752,  5.7073,  4.4484,  3.0590,  4.9073,  6.3169,  3.2420,\n",
       "          5.3813,  7.2251,  6.1880,  4.7909,  4.9241,  3.2188,  3.2178,  4.5531,\n",
       "          4.3528,  3.0726,  5.3835,  7.2055,  3.0513,  5.4377,  5.3534,  4.5734,\n",
       "          4.7762,  3.1707,  6.1003,  4.8795,  7.0466,  3.0703,  5.6894,  4.9984,\n",
       "          3.4053,  3.6234,  3.4476,  3.4673,  5.9048,  4.1396,  6.3345,  3.7195,\n",
       "          3.4640,  3.3292,  5.9234,  6.0312,  3.4081,  4.8198,  7.1394,  3.2795,\n",
       "          4.6896,  5.6283,  3.4548,  5.5129,  6.9326,  3.5910,  6.7530,  6.5020,\n",
       "          3.5922,  4.8425,  7.0781,  3.4773,  3.4999,  5.3900,  3.4720,  3.6062,\n",
       "          3.4852,  5.8576,  6.8381,  3.4521,  3.4974,  3.5147,  3.4617,  5.7755,\n",
       "          5.0025,  6.2696,  3.4640,  3.6346,  6.7850,  3.4300,  3.4473,  5.9826,\n",
       "          4.0647,  3.3964,  4.5001,  4.4077,  6.0833,  7.6945,  7.6540,  3.7765,\n",
       "          5.3187,  3.3382,  4.6371,  3.3527,  5.0562,  5.9018,  6.7612,  7.5459,\n",
       "          4.5773,  7.6864,  3.4149,  3.3271,  5.7707,  4.4854,  6.3299,  5.0059,\n",
       "          6.2123,  4.5927,  5.0594,  3.2912,  3.2374,  5.3289,  5.0393,  3.2807,\n",
       "          3.4526,  5.9351,  3.2720,  5.7481,  7.3108,  5.0537,  3.4647,  5.8870,\n",
       "          7.1496,  5.8867,  4.8734,  4.2009,  3.1387,  6.9652,  3.3442],\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18026, 4507)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder_params = dict(\n",
    "    embeddings_noise=0.005,\n",
    "    numeric_values={'transaction_amt': 'identity'},\n",
    "    embeddings={\n",
    "        'mcc_code': {'in': 377, 'out': 32},\n",
    "    },\n",
    ")\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ")\n",
    "\n",
    "model = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = PtlsDataModule(\n",
    "    train_data=ColesDataset(\n",
    "        MemoryMapDataset(\n",
    "            data=train,\n",
    "            i_filters=[\n",
    "                SeqLenFilter(min_seq_len=40),\n",
    "            ],\n",
    "        ),\n",
    "        splitter=SampleSlices(\n",
    "            split_count=5,\n",
    "            cnt_min=40,\n",
    "            cnt_max=80,\n",
    "        ),\n",
    "    ),\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belousov/miniconda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dl.setup('fit')\n",
    "dl = train_dl.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ptls.data_load.padded_batch.PaddedBatch at 0x7f27b200a100>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belousov/miniconda/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Missing logger folder: /app/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 235 K \n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "235 K     Trainable params\n",
      "0         Non-trainable params\n",
      "235 K     Total params\n",
      "0.943     Total estimated model params size (MB)\n",
      "/home/belousov/miniconda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ed96fbb4244555a851937cea3cfd85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belousov/miniconda/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
